{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad39a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data aggregation process (Location Only)...\n",
      "Loading raw data from: dataset/5GDL.csv\n",
      "Loaded 690188 rows.\n",
      "Cleaning data and converting types (Time column ignored)...\n",
      "Time column dropped.\n",
      "Dropped 617710 rows with invalid Lat or Lon.\n",
      "Sorting data by Latitude, Longitude...\n",
      "Iterating through sorted data to find static groups (Location Only)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347d22e176db40a9a54a1cfe9c07dbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Rows:   0%|          | 0/72478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation complete. Original rows (after Lat/Lon cleaning): 72478, Merged rows: 1514\n",
      "Max rows merged into one location: 558.0\n",
      "Avg rows merged (for merged points): 47.87\n",
      "WARNING: Very large groups detected based on location merge. Results might combine data from distant times. Inspect rows with high 'merged_row_count'.\n",
      "Saving merged data to: result/merged_location_only_fingerprints.csv\n",
      "Save successful.\n",
      "Total execution time: 10.21 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm # Use tqdm.auto\n",
    "\n",
    "# --- Configuration ---\n",
    "RAW_DATA_FILE = 'dataset/5GDL.csv'\n",
    "OUTPUT_FILE = 'result/merged_location_only_fingerprints.csv'\n",
    "RESULTS_DIR = 'result'\n",
    "\n",
    "# --- Column Names (!!!--ADJUST THESE TO MATCH YOUR CSV--!!!) ---\n",
    "# TIME_COL = 'Time' # REMOVED\n",
    "LAT_COL = 'Latitude'\n",
    "LON_COL = 'Longitude'\n",
    "# List ALL other columns you want to potentially merge/keep (NO Time column here)\n",
    "PARAMETER_COLS = [\n",
    "    'Technology_Mode', 'NR_UE_PCI_0', 'NR_UE_RSRP_0', 'NR_UE_RSRQ_0', 'NR_UE_SINR_0',\n",
    "    'NR_UE_Nbr_PCI_0', 'NR_UE_Nbr_PCI_1', 'NR_UE_Nbr_PCI_2', 'NR_UE_Nbr_PCI_3', 'NR_UE_Nbr_PCI_4',\n",
    "    'NR_UE_Nbr_RSRP_0', 'NR_UE_Nbr_RSRP_1', 'NR_UE_Nbr_RSRP_2', 'NR_UE_Nbr_RSRP_3', 'NR_UE_Nbr_RSRP_4',\n",
    "    'NR_UE_Nbr_RSRQ_0', 'NR_UE_Nbr_RSRQ_1', 'NR_UE_Nbr_RSRQ_2', 'NR_UE_Nbr_RSRQ_3', 'NR_UE_Nbr_RSRQ_4',\n",
    "    'NR_UE_Timing_Advance', 'NR_UE_Pathloss_DL_0', 'NR_UE_Throughput_PDCP_DL', 'App_Throughput_DL',\n",
    "    'NR_UE_NACK_Rate_DL_0', 'NR_UE_Ack_As_Nack_DL_0', 'NR_UE_MCS_DL_0', 'NR_UE_RB_Num_DL_0',\n",
    "    'NR_UE_Modulation_Avg_DL_0', 'NR_UE_RI_DL_0', 'NR_UE_BLER_DL_0', 'NR_UE_CCE_AggregationLev_0',\n",
    "    'NR_UE_Power_Tx_PUSCH_0', 'NR_UE_Power_Tx_PRACH_0', 'NR_UE_NACK_Rate_UL_0',\n",
    "    'NR_UE_RACH_Attempt', 'NR_UE_RACH_OK', 'NR_UE_RACH_Fail', 'NR_UE_RACH_Procedure_Count',\n",
    "    'NR_UE_RRCReEstAttempt', 'NR_UE_RRCReEstFail', 'NR_UE_RRCReEst_EndResult',\n",
    "    'NR_UE_RRCConnectionAttempt', 'NR_UE_RRCConnectionSetupOk', 'NR_UE_RRCConnectionComplete', # Removed extra comma\n",
    "    'NR_UE_RRCConnectionDrop', 'NR_UE_RRCHOAttempt', 'NR_UE_RRCHOOK',\n",
    "    'NR_RRC_MsgType', 'NAS_5GS_MM_MessageType', 'NAS_5GS_SM_MessageType'\n",
    "    # Add any other columns present in your raw file that you want to keep/merge\n",
    "]\n",
    "\n",
    "# --- Aggregation Parameters ---\n",
    "# MAX_TIME_DIFF_SECONDS = 2.0 # REMOVED\n",
    "\n",
    "# --- Helper Function ---\n",
    "def aggregate_static_points_no_time(df_sorted, lat_col, lon_col, param_cols):\n",
    "    \"\"\" Aggregates rows with the exact same Lat/Lon. \"\"\"\n",
    "    aggregated_data = []\n",
    "    passthrough_indices = []\n",
    "    current_group_indices = []\n",
    "    group_lat = np.nan\n",
    "    group_lon = np.nan\n",
    "\n",
    "    print(\"Iterating through sorted data to find static groups (Location Only)...\")\n",
    "    for i in tqdm(range(len(df_sorted)), desc=\"Processing Rows\"):\n",
    "        row = df_sorted.iloc[i]\n",
    "        current_lat = row[lat_col]\n",
    "        current_lon = row[lon_col]\n",
    "\n",
    "        is_same_location = (current_lat == group_lat) and (current_lon == group_lon)\n",
    "\n",
    "        # --- Check if current row continues the existing group ---\n",
    "        if is_same_location:\n",
    "            current_group_indices.append(i)\n",
    "        else:\n",
    "            # --- End of the previous group, process it ---\n",
    "            if len(current_group_indices) > 1: # Only aggregate if group has > 1 record\n",
    "                group_df = df_sorted.iloc[current_group_indices]\n",
    "                agg_result = {}\n",
    "                first_row_in_group = group_df.iloc[0]\n",
    "                agg_result[lat_col] = first_row_in_group[lat_col]\n",
    "                agg_result[lon_col] = first_row_in_group[lon_col]\n",
    "                # ADD A COUNT COLUMN TO SEE HOW MANY ROWS WERE MERGED\n",
    "                agg_result['merged_row_count'] = len(group_df)\n",
    "\n",
    "                for col in param_cols:\n",
    "                    # Handle case where column might not exist in the specific group_df (if passthrough)\n",
    "                    if col in group_df.columns:\n",
    "                        first_valid_value = group_df[col].dropna().iloc[0] if not group_df[col].dropna().empty else np.nan\n",
    "                        agg_result[col] = first_valid_value\n",
    "                    else:\n",
    "                         agg_result[col] = np.nan # Column didn't exist in this group\n",
    "                aggregated_data.append(agg_result)\n",
    "            elif len(current_group_indices) == 1:\n",
    "                passthrough_indices.append(current_group_indices[0])\n",
    "\n",
    "            # --- Start a new group with the current row ---\n",
    "            current_group_indices = [i]\n",
    "            group_lat = current_lat\n",
    "            group_lon = current_lon\n",
    "\n",
    "    # --- Process the very last group ---\n",
    "    if len(current_group_indices) > 1:\n",
    "        group_df = df_sorted.iloc[current_group_indices]\n",
    "        agg_result = {}\n",
    "        first_row_in_group = group_df.iloc[0]\n",
    "        agg_result[lat_col] = first_row_in_group[lat_col]\n",
    "        agg_result[lon_col] = first_row_in_group[lon_col]\n",
    "        agg_result['merged_row_count'] = len(group_df) # Add count\n",
    "        for col in param_cols:\n",
    "             if col in group_df.columns:\n",
    "                first_valid_value = group_df[col].dropna().iloc[0] if not group_df[col].dropna().empty else np.nan\n",
    "                agg_result[col] = first_valid_value\n",
    "             else:\n",
    "                 agg_result[col] = np.nan\n",
    "        aggregated_data.append(agg_result)\n",
    "    elif len(current_group_indices) == 1:\n",
    "        passthrough_indices.append(current_group_indices[0])\n",
    "\n",
    "    # --- Combine aggregated data and pass-through data ---\n",
    "    df_aggregated = pd.DataFrame(aggregated_data)\n",
    "    df_passthrough = df_sorted.loc[passthrough_indices].copy() # Make a copy\n",
    "    # Add the count column to passthrough rows\n",
    "    if not df_passthrough.empty:\n",
    "        df_passthrough['merged_row_count'] = 1\n",
    "\n",
    "    # Ensure column order consistency\n",
    "    # Define order based on aggregated df columns which includes the new count col\n",
    "    if not df_aggregated.empty:\n",
    "        all_cols_order = df_aggregated.columns.tolist()\n",
    "    elif not df_passthrough.empty:\n",
    "        all_cols_order = df_passthrough.columns.tolist()\n",
    "    else:\n",
    "        all_cols_order = [] # Should not happen if input wasn't empty\n",
    "\n",
    "\n",
    "    # Filter columns that actually exist in the dataframes before concat\n",
    "    agg_cols = [col for col in all_cols_order if col in df_aggregated.columns]\n",
    "    pass_cols = [col for col in all_cols_order if col in df_passthrough.columns]\n",
    "\n",
    "    df_final = pd.concat([df_aggregated[agg_cols], df_passthrough[pass_cols]], ignore_index=True)\n",
    "\n",
    "    # Re-sort by original index maybe? Or Lat/Lon? Time is gone.\n",
    "    # Sorting by Lat/Lon keeps aggregated points together.\n",
    "    df_final = df_final.sort_values(by=[LAT_COL, LON_COL]).reset_index(drop=True)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# --- Main Execution (Modified) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # ... (Load Data - consider using na_values and low_memory=False) ...\n",
    "    print(f\"Starting data aggregation process (Location Only)...\")\n",
    "    start_run_time = time.time()\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading raw data from: {RAW_DATA_FILE}\")\n",
    "        # Specify NA values and disable low_memory if DTypeWarnings persist\n",
    "        common_na = [\"N/A\", \"-\", \"--\", \"null\", \"\", \" \"]\n",
    "        df_raw = pd.read_csv(RAW_DATA_FILE, na_values=common_na, low_memory=False)\n",
    "        print(f\"Loaded {len(df_raw)} rows.\")\n",
    "    except FileNotFoundError: # ... handle errors ...\n",
    "        print(f\"ERROR: Raw data file not found at {RAW_DATA_FILE}\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load raw data. {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    print(\"Cleaning data and converting types (Time column ignored)...\")\n",
    "    # --- Drop Time column ---\n",
    "    if 'Time' in df_raw.columns:\n",
    "        df_raw = df_raw.drop(columns=['Time'])\n",
    "        print(\"Time column dropped.\")\n",
    "\n",
    "    df_raw[LAT_COL] = pd.to_numeric(df_raw[LAT_COL], errors='coerce')\n",
    "    df_raw[LON_COL] = pd.to_numeric(df_raw[LON_COL], errors='coerce')\n",
    "\n",
    "    initial_rows = len(df_raw)\n",
    "    df_raw.dropna(subset=[LAT_COL, LON_COL], inplace=True) # Drop only based on Lat/Lon\n",
    "    if len(df_raw) < initial_rows:\n",
    "        print(f\"Dropped {initial_rows - len(df_raw)} rows with invalid Lat or Lon.\")\n",
    "\n",
    "    if df_raw.empty: #... handle empty df ...\n",
    "        print(\"ERROR: No valid data remaining after cleaning Lat/Lon.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # --- Sort Data (No Time) ---\n",
    "    print(f\"Sorting data by {LAT_COL}, {LON_COL}...\")\n",
    "    df_sorted = df_raw.sort_values(by=[LAT_COL, LON_COL]).reset_index(drop=True)\n",
    "\n",
    "    # --- Perform Aggregation (No Time) ---\n",
    "    df_merged = aggregate_static_points_no_time(df_sorted, LAT_COL, LON_COL, PARAMETER_COLS)\n",
    "\n",
    "    print(f\"Aggregation complete. Original rows (after Lat/Lon cleaning): {len(df_raw)}, Merged rows: {len(df_merged)}\")\n",
    "\n",
    "    # --- Add check for large merge counts ---\n",
    "    if 'merged_row_count' in df_merged.columns:\n",
    "        max_merged = df_merged['merged_row_count'].max()\n",
    "        avg_merged = df_merged[df_merged['merged_row_count'] > 1]['merged_row_count'].mean()\n",
    "        print(f\"Max rows merged into one location: {max_merged}\")\n",
    "        print(f\"Avg rows merged (for merged points): {avg_merged:.2f}\")\n",
    "        if max_merged > 100: # Arbitrary threshold\n",
    "             print(\"WARNING: Very large groups detected based on location merge. Results might combine data from distant times. Inspect rows with high 'merged_row_count'.\")\n",
    "\n",
    "\n",
    "    # --- Save Result ---\n",
    "    # ... (Save df_merged to OUTPUT_FILE) ...\n",
    "    try:\n",
    "        print(f\"Saving merged data to: {OUTPUT_FILE}\")\n",
    "        # Ensure PARAMETER_COLS used for saving are correct\n",
    "        save_cols = [LAT_COL, LON_COL, 'merged_row_count'] + [col for col in PARAMETER_COLS if col in df_merged.columns]\n",
    "        df_merged[save_cols].to_csv(OUTPUT_FILE, index=False)\n",
    "        print(\"Save successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save merged data. {e}\")\n",
    "\n",
    "    end_run_time = time.time() #... calculate duration ...\n",
    "    print(f\"Total execution time: {end_run_time - start_run_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98968a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Signal Data Processing ---\n",
      "\n",
      "[Step 1/6] Loading data from 'dataset/5GDL.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_7480\\3890240619.py:77: DtypeWarning: Columns (1,4,30,32,33,35,45,53,54,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(CSV_FILE_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully loaded CSV with 690188 rows and 56 columns.\n",
      "  Columns found: ['Message', 'Time', 'Longitude', 'Latitude', 'Technology_Mode', 'NR_UE_PCI_0', 'NR_UE_RSRP_0', 'NR_UE_RSRQ_0', 'NR_UE_SINR_0', 'NR_UE_Nbr_PCI_0', 'NR_UE_Nbr_PCI_1', 'NR_UE_Nbr_PCI_2', 'NR_UE_Nbr_PCI_3', 'NR_UE_Nbr_PCI_4', 'NR_UE_Nbr_RSRP_0', 'NR_UE_Nbr_RSRP_1', 'NR_UE_Nbr_RSRP_2', 'NR_UE_Nbr_RSRP_3', 'NR_UE_Nbr_RSRP_4', 'NR_UE_Nbr_RSRQ_0', 'NR_UE_Nbr_RSRQ_1', 'NR_UE_Nbr_RSRQ_2', 'NR_UE_Nbr_RSRQ_3', 'NR_UE_Nbr_RSRQ_4', 'NR_UE_Timing_Advance', 'NR_UE_Pathloss_DL_0', 'NR_UE_Throughput_PDCP_DL', 'App_Throughput_DL', 'NR_UE_NACK_Rate_DL_0', 'NR_UE_Ack_As_Nack_DL_0', 'NR_UE_MCS_DL_0', 'NR_UE_RB_Num_DL_0', 'NR_UE_Modulation_Avg_DL_0', 'NR_UE_RI_DL_0', 'NR_UE_BLER_DL_0', 'NR_UE_CCE_AggregationLev_0', 'NR_UE_Power_Tx_PUSCH_0', 'NR_UE_Power_Tx_PRACH_0', 'NR_UE_NACK_Rate_UL_0', 'NR_UE_RACH_Attempt', 'NR_UE_RACH_OK', 'NR_UE_RACH_Fail', 'NR_UE_RACH_Procedure_Count', 'NR_UE_RRCReEstAttempt', 'NR_UE_RRCReEstFail', 'NR_UE_RRCReEst_EndResult', 'NR_UE_RRCConnectionAttempt', 'NR_UE_RRCConnectionSetupOk', 'Unnamed: 48', 'NR_UE_RRCConnectionComplete', 'NR_UE_RRCConnectionDrop', 'NR_UE_RRCHOAttempt', 'NR_UE_RRCHOOK', 'NR_RRC_MsgType', 'NAS_5GS_MM_MessageType', 'NAS_5GS_SM_MessageType']\n",
      "\n",
      "[Step 2/6] Pre-processing data...\n",
      "  Converted 'Time' to datetime objects.\n",
      "  Converting signal columns to numeric and filling missing values...\n",
      "    Filled 688245 NaN values (originally non-numeric) in 'NR_UE_RSRP_0' with -140.0.\n",
      "    Filled 688245 NaN values (originally non-numeric) in 'NR_UE_RSRQ_0' with -120.0.\n",
      "    Filled 688255 NaN values (originally non-numeric) in 'NR_UE_SINR_0' with -20.0.\n",
      "\n",
      "[Step 3/6] Creating GeoDataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_7480\\3890240619.py:103: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[TIMESTAMP_COLUMN] = pd.to_datetime(df[TIMESTAMP_COLUMN])\n",
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_7480\\3890240619.py:129: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(default_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warning: Dropped 617710 rows with invalid or missing Latitude/Longitude values.\n",
      "  Successfully created GeoDataFrame with 72478 points.\n",
      "\n",
      "[Step 4/6] Generating static map ('signal_map_static.png')...\n",
      "  Adding basemap...\n",
      "  Basemap added successfully.\n",
      "  Static map saved as 'signal_map_static.png'.\n",
      "\n",
      "[Step 5/6] Performing Filtering and Querying...\n",
      "\n",
      "  --- Query 1: Data nearest to (40.7132, -74.0055) ---\n",
      "  Data point closest to target:\n",
      "    Actual Location (Lat, Lon): 41.10568, 29.01534\n",
      "    Distance (approx degrees): 103.021588\n",
      "    NR_UE_RSRP_0: -140.0 (Default)\n",
      "    NR_UE_RSRQ_0: -120.0 (Default)\n",
      "    NR_UE_SINR_0: -20.0 (Default)\n",
      "    Time: 2025-03-14 12:27:08\n",
      "\n",
      "  --- Query 2: Locations where NR_UE_RSRP_0 is between -100 and -90 dBm ---\n",
      "  Found 196 locations matching the criteria.\n",
      "  First 5 matching locations:\n",
      "      Latitude  Longitude  NR_UE_RSRP_0                Time\n",
      "490  41.10723   29.02949         -90.4 2025-03-14 12:14:40\n",
      "561  41.10722   29.02947         -90.1 2025-03-14 12:14:41\n",
      "633  41.10724   29.02944         -90.7 2025-03-14 12:14:42\n",
      "704  41.10725   29.02941         -91.9 2025-03-14 12:14:43\n",
      "746  41.10725   29.02941         -91.4 2025-03-14 12:14:43\n",
      "  Generating filtered static map ('signal_map_filtered.png')...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_7480\\3890240619.py:239: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  distances = gdf.geometry.distance(target_point)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Filtered map saved as 'signal_map_filtered.png'.\n",
      "\n",
      "[Step 6/6] Generating interactive map ('signal_map_interactive.html')...\n",
      "  Interactive map saved as 'signal_map_interactive.html'. Open this file in your web browser.\n",
      "\n",
      "--- Script finished ---\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script to load geospatial signal data from CSV, handle missing values,\n",
    "plot locations on static and interactive maps, and perform filtering.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "from shapely.geometry import Point\n",
    "import folium\n",
    "import numpy as np # Used implicitly by pandas, good practice to import\n",
    "import sys # For exiting script gracefully\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration Section - MODIFY THESE VALUES\n",
    "# =============================================================================\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_FILE_PATH = 'dataset/5GDL.csv' # <<< CHANGE TO YOUR CSV FILE PATH\n",
    "LAT_COLUMN = 'Latitude'           # <<< CHANGE if your latitude column name is different\n",
    "LON_COLUMN = 'Longitude'          # <<< CHANGE if your longitude column name is different\n",
    "RSRP_COLUMN = 'NR_UE_RSRP_0'              # <<< CHANGE if your RSRP column name is different\n",
    "SINR_COLUMN = 'NR_UE_SINR_0'\n",
    "TIMESTAMP_COLUMN = 'Time'\n",
    "RSSI_COLUMN = 'NR_UE_RSRQ_0'\n",
    "# Add other columns you care about\n",
    "OTHER_COLUMNS = ['NR_UE_SINR_0', 'NR_UE_Timing_Advance', '']\n",
    "\n",
    "# --- Default Values for Missing Data ---\n",
    "# Choose values unlikely to occur naturally in your data\n",
    "DEFAULT_RSRP = -140.0\n",
    "DEFAULT_RSSI = -120.0\n",
    "DEFAULT_SINR = -20.0\n",
    "# Add defaults for other columns if they might be missing and numeric\n",
    "# DEFAULT_OTHER_SIGNAL = -999.0\n",
    "\n",
    "# --- Map Settings ---\n",
    "STATIC_MAP_FILENAME = 'signal_map_static.png'\n",
    "FILTERED_MAP_FILENAME = 'signal_map_filtered.png'\n",
    "INTERACTIVE_MAP_FILENAME = 'signal_map_interactive.html'\n",
    "DEFAULT_ZOOM_START = 13 # Zoom level for Folium map\n",
    "\n",
    "# --- Filtering Parameters ---\n",
    "# Example 1: Find data near this point\n",
    "TARGET_LATITUDE = 40.7132\n",
    "TARGET_LONGITUDE = -74.0055\n",
    "\n",
    "# Example 2: Filter by RSRP range\n",
    "MIN_RSRP_THRESHOLD = -100\n",
    "MAX_RSRP_THRESHOLD = -90\n",
    "\n",
    "# =============================================================================\n",
    "# Helper Function for Robust Numeric Conversion\n",
    "# =============================================================================\n",
    "def safe_to_numeric(series, column_name):\n",
    "    \"\"\"Converts a pandas Series to numeric, coercing errors, and reports issues.\"\"\"\n",
    "    original_dtype = series.dtype\n",
    "    numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "    num_nan_introduced = numeric_series.isna().sum() - series.isna().sum()\n",
    "    if num_nan_introduced > 0:\n",
    "        print(f\"  Warning: Column '{column_name}' had {num_nan_introduced} non-numeric values coerced to NaN.\")\n",
    "    if not pd.api.types.is_numeric_dtype(numeric_series) and not numeric_series.isna().all():\n",
    "         # This case is rare after 'coerce', but good practice\n",
    "         print(f\"  Warning: Column '{column_name}' could not be fully converted to numeric (original dtype: {original_dtype}). Check data.\")\n",
    "    return numeric_series\n",
    "\n",
    "# =============================================================================\n",
    "# Main Script Logic\n",
    "# =============================================================================\n",
    "print(\"--- Starting Signal Data Processing ---\")\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(f\"\\n[Step 1/6] Loading data from '{CSV_FILE_PATH}'...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_FILE_PATH)\n",
    "    print(f\"  Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns.\")\n",
    "    print(\"  Columns found:\", df.columns.tolist())\n",
    "    # print(\"  First 5 rows (raw):\\n\", df.head()) # Uncomment for debugging\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"  Error: CSV file not found at '{CSV_FILE_PATH}'. Please check the path.\")\n",
    "    sys.exit(1) # Exit script\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An unexpected error occurred loading the CSV: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Verify Essential Columns Exist ---\n",
    "required_columns = [LAT_COLUMN, LON_COLUMN]\n",
    "if RSRP_COLUMN: required_columns.append(RSRP_COLUMN)\n",
    "missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"  Error: Missing essential columns in CSV: {missing_cols}. Check configuration.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- 2. Pre-process Data (Timestamp, Numeric Conversion, Fill NaNs) ---\n",
    "print(\"\\n[Step 2/6] Pre-processing data...\")\n",
    "\n",
    "# Convert Timestamp\n",
    "if TIMESTAMP_COLUMN and TIMESTAMP_COLUMN in df.columns:\n",
    "    try:\n",
    "        df[TIMESTAMP_COLUMN] = pd.to_datetime(df[TIMESTAMP_COLUMN])\n",
    "        print(f\"  Converted '{TIMESTAMP_COLUMN}' to datetime objects.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not parse timestamp column '{TIMESTAMP_COLUMN}'. Error: {e}. Skipping conversion.\")\n",
    "else:\n",
    "     if TIMESTAMP_COLUMN:\n",
    "         print(f\"  Info: Timestamp column '{TIMESTAMP_COLUMN}' not found in CSV.\")\n",
    "\n",
    "# Define columns to fill and their default values\n",
    "columns_to_fill = {}\n",
    "if RSRP_COLUMN in df.columns: columns_to_fill[RSRP_COLUMN] = DEFAULT_RSRP\n",
    "if RSSI_COLUMN in df.columns: columns_to_fill[RSSI_COLUMN] = DEFAULT_RSSI\n",
    "if SINR_COLUMN in df.columns: columns_to_fill[SINR_COLUMN] = DEFAULT_SINR\n",
    "# Add other numeric columns needing defaults here:\n",
    "# if 'OtherSignal' in df.columns: columns_to_fill['OtherSignal'] = DEFAULT_OTHER_SIGNAL\n",
    "\n",
    "# Apply numeric conversion and fill NaNs\n",
    "print(\"  Converting signal columns to numeric and filling missing values...\")\n",
    "for col, default_val in columns_to_fill.items():\n",
    "    if col in df.columns:\n",
    "        nan_count_before = df[col].isna().sum()\n",
    "        # Convert to numeric first, handling potential non-numeric entries\n",
    "        df[col] = safe_to_numeric(df[col], col)\n",
    "        nan_count_after_coerce = df[col].isna().sum()\n",
    "\n",
    "        if nan_count_after_coerce > 0:\n",
    "            df[col].fillna(default_val, inplace=True)\n",
    "            filled_count = nan_count_after_coerce - nan_count_before # Count NaNs filled (original + coerced)\n",
    "            if filled_count > 0 :\n",
    "                 print(f\"    Filled {filled_count} NaN/invalid values in '{col}' with {default_val}.\")\n",
    "            else:\n",
    "                 # This happens if all NaNs were introduced by coercion and then filled\n",
    "                 if nan_count_after_coerce > 0:\n",
    "                      print(f\"    Filled {nan_count_after_coerce} NaN values (originally non-numeric) in '{col}' with {default_val}.\")\n",
    "        # else: # No NaNs found or introduced\n",
    "        #     print(f\"    No NaN values found or filled in '{col}'.\") # Can be noisy, often omitted\n",
    "    else:\n",
    "        print(f\"  Warning: Column '{col}' specified for filling not found in DataFrame.\")\n",
    "\n",
    "# print(\"  First 5 rows (after processing):\\n\", df.head()) # Uncomment for debugging\n",
    "\n",
    "# --- 3. Create GeoDataFrame ---\n",
    "print(\"\\n[Step 3/6] Creating GeoDataFrame...\")\n",
    "try:\n",
    "    # Ensure Lat/Lon are numeric, drop rows if not convertible\n",
    "    df[LAT_COLUMN] = safe_to_numeric(df[LAT_COLUMN], LAT_COLUMN)\n",
    "    df[LON_COLUMN] = safe_to_numeric(df[LON_COLUMN], LON_COLUMN)\n",
    "\n",
    "    original_rows = len(df)\n",
    "    df.dropna(subset=[LAT_COLUMN, LON_COLUMN], inplace=True)\n",
    "    rows_dropped = original_rows - len(df)\n",
    "    if rows_dropped > 0:\n",
    "        print(f\"  Warning: Dropped {rows_dropped} rows with invalid or missing Latitude/Longitude values.\")\n",
    "\n",
    "    if df.empty:\n",
    "         print(\"  Error: No valid location data remaining after cleaning Lat/Lon. Cannot proceed.\")\n",
    "         sys.exit(1)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[LON_COLUMN], df[LAT_COLUMN]),\n",
    "        crs=\"EPSG:4326\" # WGS84 Coordinate Reference System\n",
    "    )\n",
    "    print(f\"  Successfully created GeoDataFrame with {len(gdf)} points.\")\n",
    "    # print(gdf.head()) # Uncomment for debugging\n",
    "\n",
    "except KeyError as e:\n",
    "     print(f\"  Error: Missing required column for GeoDataFrame creation: {e}. Check LAT_COLUMN/LON_COLUMN names.\")\n",
    "     sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An unexpected error occurred creating the GeoDataFrame: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- 4. Plotting: Static Map ---\n",
    "print(f\"\\n[Step 4/6] Generating static map ('{STATIC_MAP_FILENAME}')...\")\n",
    "try:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "\n",
    "    # Determine color range, potentially excluding the default fill value\n",
    "    color_col = RSRP_COLUMN\n",
    "    plot_column_data = gdf[color_col]\n",
    "    default_val = columns_to_fill.get(color_col) # Get default if defined\n",
    "\n",
    "    # Calculate vmin/vmax from actual data, ignoring default fill value if specified\n",
    "    valid_data_for_range = plot_column_data\n",
    "    if default_val is not None:\n",
    "        valid_data_for_range = plot_column_data[plot_column_data != default_val]\n",
    "\n",
    "    vmin = valid_data_for_range.min() if not valid_data_for_range.empty else plot_column_data.min()\n",
    "    vmax = valid_data_for_range.max() if not valid_data_for_range.empty else plot_column_data.max()\n",
    "\n",
    "\n",
    "    gdf.plot(\n",
    "        column=color_col,\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        markersize=15,\n",
    "        cmap='viridis', # Good perceptually uniform colormap\n",
    "        vmin=vmin,      # Set color limits based on data range\n",
    "        vmax=vmax,\n",
    "        legend_kwds={'label': f\"{color_col} (dBm)\",\n",
    "                     'orientation': \"horizontal\"}\n",
    "    )\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        print(\"  Adding basemap...\")\n",
    "        ctx.add_basemap(ax, crs=gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik, zoom='auto')\n",
    "        print(\"  Basemap added successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not add basemap. Plot will show points only. Error: {e}\")\n",
    "\n",
    "    ax.set_title(f'Signal Measurement Locations colored by {color_col}')\n",
    "    ax.set_axis_off() # Hide lat/lon axes if basemap is present\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(STATIC_MAP_FILENAME, dpi=300)\n",
    "    print(f\"  Static map saved as '{STATIC_MAP_FILENAME}'.\")\n",
    "    # plt.show() # Optionally display the plot directly\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An error occurred during static plotting: {e}\")\n",
    "\n",
    "plt.close(fig) # Close the figure to free memory\n",
    "\n",
    "# --- 5. Filtering and Querying ---\n",
    "print(\"\\n[Step 5/6] Performing Filtering and Querying...\")\n",
    "\n",
    "# Example 1: Find RSRP (and other data) near a specific Lat/Lon\n",
    "print(f\"\\n  --- Query 1: Data nearest to ({TARGET_LATITUDE}, {TARGET_LONGITUDE}) ---\")\n",
    "try:\n",
    "    target_point = Point(TARGET_LONGITUDE, TARGET_LATITUDE)\n",
    "    # Ensure target point is valid\n",
    "    if not target_point.is_valid:\n",
    "         print(f\"  Error: Invalid target coordinates specified.\")\n",
    "    else:\n",
    "        # Calculate distances (in degrees for EPSG:4326)\n",
    "        distances = gdf.geometry.distance(target_point)\n",
    "\n",
    "        if not distances.empty:\n",
    "            nearest_index = distances.idxmin()\n",
    "            nearest_data = gdf.loc[nearest_index]\n",
    "\n",
    "            print(f\"  Data point closest to target:\")\n",
    "            print(f\"    Actual Location (Lat, Lon): {nearest_data.geometry.y:.5f}, {nearest_data.geometry.x:.5f}\")\n",
    "            print(f\"    Distance (approx degrees): {distances.min():.6f}\") # Note: Degree distance != meters!\n",
    "            # Display key values, checking if they exist and handling defaults\n",
    "            for col, default in columns_to_fill.items():\n",
    "                 if col in nearest_data:\n",
    "                     val = nearest_data[col]\n",
    "                     print(f\"    {col}: {val}{' (Default)' if val == default else ''}\")\n",
    "            # Display timestamp if available\n",
    "            if TIMESTAMP_COLUMN and TIMESTAMP_COLUMN in nearest_data and pd.notna(nearest_data[TIMESTAMP_COLUMN]):\n",
    "                 print(f\"    {TIMESTAMP_COLUMN}: {nearest_data[TIMESTAMP_COLUMN]}\")\n",
    "            # Display other configured columns\n",
    "            # for col in OTHER_DATA_COLUMNS:\n",
    "            #      if col in nearest_data and col not in columns_to_fill: # Avoid printing twice\n",
    "            #           print(f\"    {col}: {nearest_data[col]}\")\n",
    "        else:\n",
    "            print(f\"  No data points found to calculate nearest distance.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An error occurred finding the nearest point: {e}\")\n",
    "\n",
    "\n",
    "# Example 2: Find locations where RSRP is within a certain range\n",
    "print(f\"\\n  --- Query 2: Locations where {RSRP_COLUMN} is between {MIN_RSRP_THRESHOLD} and {MAX_RSRP_THRESHOLD} dBm ---\")\n",
    "if RSRP_COLUMN not in gdf.columns:\n",
    "     print(f\"  Skipping query: RSRP column '{RSRP_COLUMN}' not found.\")\n",
    "else:\n",
    "    try:\n",
    "        # Apply the filter using boolean indexing\n",
    "        filtered_gdf = gdf[\n",
    "            (gdf[RSRP_COLUMN] >= MIN_RSRP_THRESHOLD) &\n",
    "            (gdf[RSRP_COLUMN] <= MAX_RSRP_THRESHOLD)\n",
    "        ]\n",
    "\n",
    "        print(f\"  Found {len(filtered_gdf)} locations matching the criteria.\")\n",
    "\n",
    "        if not filtered_gdf.empty:\n",
    "            # Display first few matching locations\n",
    "            display_cols = [LAT_COLUMN, LON_COLUMN, RSRP_COLUMN]\n",
    "            if TIMESTAMP_COLUMN in filtered_gdf.columns: display_cols.append(TIMESTAMP_COLUMN)\n",
    "            print(\"  First 5 matching locations:\\n\", filtered_gdf[display_cols].head())\n",
    "\n",
    "            # Optional: Plot only the filtered points on a new map\n",
    "            print(f\"  Generating filtered static map ('{FILTERED_MAP_FILENAME}')...\")\n",
    "            try:\n",
    "                fig_filtered, ax_filtered = plt.subplots(1, 1, figsize=(10, 10))\n",
    "                # Plot all points faintly for context\n",
    "                gdf.plot(ax=ax_filtered, color='grey', markersize=5, alpha=0.3, label='All Data')\n",
    "                # Highlight filtered points\n",
    "                filtered_gdf.plot(ax=ax_filtered, color='red', markersize=25, label=f'{RSRP_COLUMN} {MIN_RSRP_THRESHOLD} to {MAX_RSRP_THRESHOLD}')\n",
    "                # Add Basemap\n",
    "                try:\n",
    "                    ctx.add_basemap(ax_filtered, crs=gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik, zoom='auto')\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: Could not add basemap to filtered plot. Error: {e}\")\n",
    "\n",
    "                ax_filtered.set_title(f'Locations with {RSRP_COLUMN} in range [{MIN_RSRP_THRESHOLD}, {MAX_RSRP_THRESHOLD}] dBm')\n",
    "                ax_filtered.set_axis_off()\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(FILTERED_MAP_FILENAME, dpi=300)\n",
    "                print(f\"  Filtered map saved as '{FILTERED_MAP_FILENAME}'.\")\n",
    "                # plt.show() # Optionally display\n",
    "                plt.close(fig_filtered) # Close figure\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: An error occurred plotting the filtered map: {e}\")\n",
    "                if 'fig_filtered' in locals(): plt.close(fig_filtered) # Ensure cleanup if error occurred mid-plot\n",
    "\n",
    "        else:\n",
    "            print(\"  No locations matched the filtering criteria.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: An error occurred during RSRP filtering: {e}\")\n",
    "\n",
    "\n",
    "# --- 6. Plotting: Interactive Map (Folium) ---\n",
    "print(f\"\\n[Step 6/6] Generating interactive map ('{INTERACTIVE_MAP_FILENAME}')...\")\n",
    "try:\n",
    "    # Create map centered around the mean location\n",
    "    map_center = [gdf[LAT_COLUMN].mean(), gdf[LON_COLUMN].mean()]\n",
    "    interactive_map = folium.Map(location=map_center, zoom_start=DEFAULT_ZOOM_START)\n",
    "\n",
    "    # Add points to the map\n",
    "    for idx, row in gdf.iterrows():\n",
    "        # Create popup text with relevant info\n",
    "        popup_html = f\"<b><u>Location {idx}</u></b><br>\"\n",
    "        popup_html += f\"<b>Lat:</b> {row[LAT_COLUMN]:.5f}<br>\"\n",
    "        popup_html += f\"<b>Lon:</b> {row[LON_COLUMN]:.5f}<br>\"\n",
    "\n",
    "        # Add signal values, indicating if it was a default fill\n",
    "        all_signal_cols = list(columns_to_fill.keys()) # Includes RSRP, RSSI, SINR etc. if defined\n",
    "        for col in all_signal_cols:\n",
    "            if col in row:\n",
    "                 val = row[col]\n",
    "                 default_val = columns_to_fill.get(col) # Get the default for comparison\n",
    "                 popup_html += f\"<b>{col}:</b> {val}{' (Default)' if val == default_val else ''}<br>\"\n",
    "\n",
    "        # Add other data columns specified\n",
    "        # for col in OTHER_DATA_COLUMNS:\n",
    "        #     if col in row and col not in columns_to_fill: # Avoid adding signals twice\n",
    "        #          popup_html += f\"<b>{col}:</b> {row[col]}<br>\"\n",
    "\n",
    "        # Add timestamp if available\n",
    "        if TIMESTAMP_COLUMN and TIMESTAMP_COLUMN in row and pd.notna(row[TIMESTAMP_COLUMN]):\n",
    "             popup_html += f\"<b>{TIMESTAMP_COLUMN}:</b> {row[TIMESTAMP_COLUMN]}\"\n",
    "\n",
    "        # Determine marker color based on RSRP (customize this logic if needed)\n",
    "        rsrp_val = row.get(RSRP_COLUMN) # Use .get for safety if RSRP_COLUMN somehow missing\n",
    "        rsrp_default = columns_to_fill.get(RSRP_COLUMN)\n",
    "        color = 'grey' # Default/Unknown\n",
    "\n",
    "        if rsrp_val is not None:\n",
    "            if rsrp_val == rsrp_default:\n",
    "                 color = 'darkgrey' # Specific color for default/missing RSRP\n",
    "            elif rsrp_val > -95:\n",
    "                 color = 'green'\n",
    "            elif rsrp_val > -105:\n",
    "                 color = 'orange'\n",
    "            elif rsrp_val <= -105: # Catches values below -105\n",
    "                 color = 'red'\n",
    "            # Add more elif conditions for finer granularity if desired\n",
    "\n",
    "        # Add marker to map\n",
    "        folium.CircleMarker(\n",
    "            location=[row[LAT_COLUMN], row[LON_COLUMN]],\n",
    "            radius=5, # Adjust size\n",
    "            popup=folium.Popup(popup_html, max_width=300),\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.7\n",
    "        ).add_to(interactive_map)\n",
    "\n",
    "    # Save the map to an HTML file\n",
    "    interactive_map.save(INTERACTIVE_MAP_FILENAME)\n",
    "    print(f\"  Interactive map saved as '{INTERACTIVE_MAP_FILENAME}'. Open this file in your web browser.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An error occurred generating the interactive map: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Script finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
