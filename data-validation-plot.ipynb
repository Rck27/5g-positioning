{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad39a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data aggregation process...\n",
      "Loading raw data from: dataset/5GDL.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_8088\\1054160611.py:164: DtypeWarning: Columns (1,4,30,32,33,35,45,53,54,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_raw = pd.read_csv(RAW_DATA_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 690188 rows.\n",
      "Cleaning data and converting types...\n",
      "Warning: Format '%d/%b/%y %H:%M:%S' failed for many entries. Trying pandas inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_8088\\1054160611.py:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  converted_infer = pd.to_datetime(series, errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original format results kept despite errors.\n",
      "Dropped 617710 rows with invalid Time, Lat, or Lon.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'NAME_PATTERN'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_8088\\1054160611.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# This helps catch errors early if data isn't as expected\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;28;01min\u001b[39;00m PARAMETER_COLS:\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;28;01min\u001b[39;00m df_raw.columns:\n\u001b[32m    197\u001b[39m              \u001b[38;5;66;03m# Heuristic: If column seems mostly numeric attempt conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m              \u001b[38;5;28;01mif\u001b[39;00m df_raw[col]. NAME_PATTERN(\u001b[33mr'^-?\\d+(\\.\\d+)?$'\u001b[39m).all(): \u001b[38;5;66;03m# Check if values look like numbers\u001b[39;00m\n\u001b[32m    199\u001b[39m                  df_raw[col] = pd.to_numeric(df_raw[col], errors=\u001b[33m'coerce'\u001b[39m)\n\u001b[32m    200\u001b[39m         \u001b[38;5;66;03m# else: Keep as object/string if not numeric-like or if column doesn't exist\u001b[39;00m\n\u001b[32m    201\u001b[39m \n",
      "\u001b[32mc:\\Users\\deeric\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6295\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6296\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6297\u001b[39m         ):\n\u001b[32m   6298\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6299\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Series' object has no attribute 'NAME_PATTERN'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm # Use tqdm.auto for broader compatibility\n",
    "\n",
    "# --- Configuration ---\n",
    "RAW_DATA_FILE = 'dataset/5GDL.csv'  # Your input file\n",
    "OUTPUT_FILE = 'result/merged_location_fingerprints.csv' # Output file\n",
    "RESULTS_DIR = 'result'\n",
    "\n",
    "# --- Column Names (!!!--ADJUST THESE TO MATCH YOUR CSV--!!!) ---\n",
    "TIME_COL = 'Time'\n",
    "LAT_COL = 'Latitude'\n",
    "LON_COL = 'Longitude'\n",
    "# List ALL other columns you want to potentially merge/keep\n",
    "# It's important to list columns you expect might be non-null in different rows of a group\n",
    "PARAMETER_COLS = [\n",
    "    'Technology_Mode', 'NR_UE_PCI_0', 'NR_UE_RSRP_0', 'NR_UE_RSRQ_0', 'NR_UE_SINR_0',\n",
    "    'NR_UE_Nbr_PCI_0', 'NR_UE_Nbr_PCI_1', 'NR_UE_Nbr_PCI_2', 'NR_UE_Nbr_PCI_3', 'NR_UE_Nbr_PCI_4',\n",
    "    'NR_UE_Nbr_RSRP_0', 'NR_UE_Nbr_RSRP_1', 'NR_UE_Nbr_RSRP_2', 'NR_UE_Nbr_RSRP_3', 'NR_UE_Nbr_RSRP_4',\n",
    "    'NR_UE_Nbr_RSRQ_0', 'NR_UE_Nbr_RSRQ_1', 'NR_UE_Nbr_RSRQ_2', 'NR_UE_Nbr_RSRQ_3', 'NR_UE_Nbr_RSRQ_4',\n",
    "    'NR_UE_Timing_Advance', 'NR_UE_Pathloss_DL_0', 'NR_UE_Throughput_PDCP_DL', 'App_Throughput_DL',\n",
    "    'NR_UE_NACK_Rate_DL_0', 'NR_UE_Ack_As_Nack_DL_0', 'NR_UE_MCS_DL_0', 'NR_UE_RB_Num_DL_0',\n",
    "    'NR_UE_Modulation_Avg_DL_0', 'NR_UE_RI_DL_0', 'NR_UE_BLER_DL_0', 'NR_UE_CCE_AggregationLev_0',\n",
    "    'NR_UE_Power_Tx_PUSCH_0', 'NR_UE_Power_Tx_PRACH_0', 'NR_UE_NACK_Rate_UL_0',\n",
    "    'NR_UE_RACH_Attempt', 'NR_UE_RACH_OK', 'NR_UE_RACH_Fail', 'NR_UE_RACH_Procedure_Count',\n",
    "    'NR_UE_RRCReEstAttempt', 'NR_UE_RRCReEstFail', 'NR_UE_RRCReEst_EndResult',\n",
    "    'NR_UE_RRCConnectionAttempt', 'NR_UE_RRCConnectionSetupOk', 'NR_UE_RRCConnectionComplete', # Removed extra comma\n",
    "    'NR_UE_RRCConnectionDrop', 'NR_UE_RRCHOAttempt', 'NR_UE_RRCHOOK',\n",
    "    'NR_RRC_MsgType', 'NAS_5GS_MM_MessageType', 'NAS_5GS_SM_MessageType'\n",
    "    # Add any other columns present in your raw file that you want to keep/merge\n",
    "]\n",
    "\n",
    "# --- Aggregation Parameters ---\n",
    "# Max time difference (in seconds) between first and last record in a \"static group\"\n",
    "MAX_TIME_DIFF_SECONDS = 2.0 # Based on your finding of 84 records in 2s\n",
    "\n",
    "# --- Helper Function ---\n",
    "def safe_convert_to_datetime(series, date_format=\"%d/%b/%y %H:%M:%S\"):\n",
    "    \"\"\"Attempts to convert Series to datetime, handling potential errors.\"\"\"\n",
    "    # Try the primary format first\n",
    "    converted = pd.to_datetime(series, format=date_format, errors='coerce')\n",
    "    # If many NaTs, try letting pandas infer (slower but more flexible)\n",
    "    if converted.isnull().sum() > 0.1 * len(series): # Heuristic: if >10% failed\n",
    "        print(f\"Warning: Format '{date_format}' failed for many entries. Trying pandas inference...\")\n",
    "        converted_infer = pd.to_datetime(series, errors='coerce')\n",
    "        # Only use inferred if it resulted in fewer NaTs\n",
    "        if converted_infer.isnull().sum() < converted.isnull().sum():\n",
    "            print(\"Pandas inference yielded better results.\")\n",
    "            return converted_infer\n",
    "        else:\n",
    "            print(\"Original format results kept despite errors.\")\n",
    "            return converted\n",
    "    return converted\n",
    "\n",
    "def aggregate_static_points(df_sorted, lat_col, lon_col, time_col, param_cols, max_time_diff):\n",
    "    \"\"\"\n",
    "    Aggregates rows with the same Lat/Lon within a time window.\n",
    "\n",
    "    Args:\n",
    "        df_sorted (pd.DataFrame): DataFrame sorted by Lat, Lon, Time.\n",
    "        lat_col (str): Latitude column name.\n",
    "        lon_col (str): Longitude column name.\n",
    "        time_col (str): Datetime column name.\n",
    "        param_cols (list): List of other column names to aggregate.\n",
    "        max_time_diff (pd.Timedelta): Maximum time difference for a group.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with aggregated static points and original moving points.\n",
    "    \"\"\"\n",
    "    aggregated_data = []\n",
    "    passthrough_indices = []\n",
    "    current_group_indices = []\n",
    "    group_start_time = pd.NaT\n",
    "    group_lat = np.nan\n",
    "    group_lon = np.nan\n",
    "\n",
    "    print(\"Iterating through sorted data to find static groups...\")\n",
    "    for i in tqdm(range(len(df_sorted)), desc=\"Processing Rows\"):\n",
    "        row = df_sorted.iloc[i]\n",
    "        current_lat = row[lat_col]\n",
    "        current_lon = row[lon_col]\n",
    "        current_time = row[time_col]\n",
    "\n",
    "        is_same_location = (current_lat == group_lat) and (current_lon == group_lon)\n",
    "        is_within_time = pd.notna(group_start_time) and (current_time - group_start_time <= max_time_diff)\n",
    "\n",
    "        # --- Check if current row continues the existing group ---\n",
    "        if is_same_location and is_within_time:\n",
    "            current_group_indices.append(i)\n",
    "        else:\n",
    "            # --- End of the previous group, process it ---\n",
    "            if len(current_group_indices) > 1: # Only aggregate if group has more than 1 record\n",
    "                # Aggregate the completed group\n",
    "                group_df = df_sorted.iloc[current_group_indices]\n",
    "                agg_result = {}\n",
    "                # Keep first row's identity info\n",
    "                first_row_in_group = group_df.iloc[0]\n",
    "                agg_result[lat_col] = first_row_in_group[lat_col]\n",
    "                agg_result[lon_col] = first_row_in_group[lon_col]\n",
    "                agg_result[time_col] = first_row_in_group[time_col] # Time of the start of the static event\n",
    "\n",
    "                # Aggregate other parameters: find first non-null value in the group\n",
    "                for col in param_cols:\n",
    "                    first_valid_value = group_df[col].dropna().iloc[0] if not group_df[col].dropna().empty else np.nan\n",
    "                    agg_result[col] = first_valid_value\n",
    "                aggregated_data.append(agg_result)\n",
    "\n",
    "            elif len(current_group_indices) == 1:\n",
    "                # Pass through single-row groups (or first row of a potential group that broke immediately)\n",
    "                passthrough_indices.append(current_group_indices[0])\n",
    "\n",
    "            # --- Start a new group with the current row ---\n",
    "            current_group_indices = [i]\n",
    "            group_start_time = current_time\n",
    "            group_lat = current_lat\n",
    "            group_lon = current_lon\n",
    "\n",
    "    # --- Process the very last group after the loop ---\n",
    "    if len(current_group_indices) > 1:\n",
    "        group_df = df_sorted.iloc[current_group_indices]\n",
    "        agg_result = {}\n",
    "        first_row_in_group = group_df.iloc[0]\n",
    "        agg_result[lat_col] = first_row_in_group[lat_col]\n",
    "        agg_result[lon_col] = first_row_in_group[lon_col]\n",
    "        agg_result[time_col] = first_row_in_group[time_col]\n",
    "        for col in param_cols:\n",
    "            first_valid_value = group_df[col].dropna().iloc[0] if not group_df[col].dropna().empty else np.nan\n",
    "            agg_result[col] = first_valid_value\n",
    "        aggregated_data.append(agg_result)\n",
    "    elif len(current_group_indices) == 1:\n",
    "        passthrough_indices.append(current_group_indices[0])\n",
    "\n",
    "    # --- Combine aggregated data and pass-through data ---\n",
    "    df_aggregated = pd.DataFrame(aggregated_data)\n",
    "    df_passthrough = df_sorted.loc[passthrough_indices]\n",
    "\n",
    "    # Ensure column order consistency (use original df columns + any new ones if needed)\n",
    "    all_cols_order = [lat_col, lon_col, time_col] + param_cols\n",
    "    # Filter columns that actually exist in the dataframes\n",
    "    agg_cols = [col for col in all_cols_order if col in df_aggregated.columns]\n",
    "    pass_cols = [col for col in all_cols_order if col in df_passthrough.columns]\n",
    "\n",
    "    df_final = pd.concat([df_aggregated[agg_cols], df_passthrough[pass_cols]], ignore_index=True)\n",
    "\n",
    "    # Optional: Sort final result by time again\n",
    "    df_final = df_final.sort_values(by=time_col).reset_index(drop=True)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting data aggregation process...\")\n",
    "    start_run_time = time.time()\n",
    "\n",
    "    # --- Ensure results directory exists ---\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Load Data ---\n",
    "    try:\n",
    "        print(f\"Loading raw data from: {RAW_DATA_FILE}\")\n",
    "        df_raw = pd.read_csv(RAW_DATA_FILE)\n",
    "        print(f\"Loaded {len(df_raw)} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Raw data file not found at {RAW_DATA_FILE}\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load raw data. {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- Basic Cleaning & Type Conversion ---\n",
    "    print(\"Cleaning data and converting types...\")\n",
    "    # Time Conversion (adjust format string if necessary)\n",
    "    # Handle potential extra spaces or slightly off formats\n",
    "    df_raw[TIME_COL] = df_raw[TIME_COL].astype(str).str.strip()\n",
    "    df_raw[TIME_COL] = safe_convert_to_datetime(df_raw[TIME_COL]) # Use safe conversion\n",
    "    # Location Conversion\n",
    "    df_raw[LAT_COL] = pd.to_numeric(df_raw[LAT_COL], errors='coerce')\n",
    "    df_raw[LON_COL] = pd.to_numeric(df_raw[LON_COL], errors='coerce')\n",
    "\n",
    "    # Drop rows where essential info (Time, Lat, Lon) is invalid AFTER conversion attempt\n",
    "    initial_rows = len(df_raw)\n",
    "    df_raw.dropna(subset=[TIME_COL, LAT_COL, LON_COL], inplace=True)\n",
    "    if len(df_raw) < initial_rows:\n",
    "        print(f\"Dropped {initial_rows - len(df_raw)} rows with invalid Time, Lat, or Lon.\")\n",
    "\n",
    "    if df_raw.empty:\n",
    "        print(\"ERROR: No valid data remaining after cleaning Time/Lat/Lon.\")\n",
    "        exit()\n",
    "\n",
    "    # Convert parameter columns to appropriate types (mostly numeric, object for text ones)\n",
    "    # This helps catch errors early if data isn't as expected\n",
    "    for col in PARAMETER_COLS:\n",
    "        if col in df_raw.columns:\n",
    "             # Heuristic: If column seems mostly numeric attempt conversion\n",
    "             if df_raw[col]. NAME_PATTERN(r'^-?\\d+(\\.\\d+)?$').all(): # Check if values look like numbers\n",
    "                 df_raw[col] = pd.to_numeric(df_raw[col], errors='coerce')\n",
    "        # else: Keep as object/string if not numeric-like or if column doesn't exist\n",
    "\n",
    "\n",
    "    # --- Sort Data ---\n",
    "    print(f\"Sorting data by {LAT_COL}, {LON_COL}, {TIME_COL}...\")\n",
    "    df_sorted = df_raw.sort_values(by=[LAT_COL, LON_COL, TIME_COL]).reset_index(drop=True)\n",
    "\n",
    "    # --- Perform Aggregation ---\n",
    "    max_diff_timedelta = pd.Timedelta(seconds=MAX_TIME_DIFF_SECONDS)\n",
    "    df_merged = aggregate_static_points(df_sorted, LAT_COL, LON_COL, TIME_COL, PARAMETER_COLS, max_diff_timedelta)\n",
    "\n",
    "    print(f\"Aggregation complete. Original rows: {len(df_raw)}, Merged rows: {len(df_merged)}\")\n",
    "\n",
    "    # --- Save Result ---\n",
    "    try:\n",
    "        print(f\"Saving merged data to: {OUTPUT_FILE}\")\n",
    "        df_merged.to_csv(OUTPUT_FILE, index=False, date_format='%Y-%m-%d %H:%M:%S.%f') # Use standard format\n",
    "        print(\"Save successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save merged data. {e}\")\n",
    "\n",
    "    end_run_time = time.time()\n",
    "    print(f\"Total execution time: {end_run_time - start_run_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98968a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Signal Data Processing ---\n",
      "\n",
      "[Step 1/6] Loading data from 'dataset/5GDL.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_7480\\3890240619.py:77: DtypeWarning: Columns (1,4,30,32,33,35,45,53,54,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(CSV_FILE_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully loaded CSV with 690188 rows and 56 columns.\n",
      "  Columns found: ['Message', 'Time', 'Longitude', 'Latitude', 'Technology_Mode', 'NR_UE_PCI_0', 'NR_UE_RSRP_0', 'NR_UE_RSRQ_0', 'NR_UE_SINR_0', 'NR_UE_Nbr_PCI_0', 'NR_UE_Nbr_PCI_1', 'NR_UE_Nbr_PCI_2', 'NR_UE_Nbr_PCI_3', 'NR_UE_Nbr_PCI_4', 'NR_UE_Nbr_RSRP_0', 'NR_UE_Nbr_RSRP_1', 'NR_UE_Nbr_RSRP_2', 'NR_UE_Nbr_RSRP_3', 'NR_UE_Nbr_RSRP_4', 'NR_UE_Nbr_RSRQ_0', 'NR_UE_Nbr_RSRQ_1', 'NR_UE_Nbr_RSRQ_2', 'NR_UE_Nbr_RSRQ_3', 'NR_UE_Nbr_RSRQ_4', 'NR_UE_Timing_Advance', 'NR_UE_Pathloss_DL_0', 'NR_UE_Throughput_PDCP_DL', 'App_Throughput_DL', 'NR_UE_NACK_Rate_DL_0', 'NR_UE_Ack_As_Nack_DL_0', 'NR_UE_MCS_DL_0', 'NR_UE_RB_Num_DL_0', 'NR_UE_Modulation_Avg_DL_0', 'NR_UE_RI_DL_0', 'NR_UE_BLER_DL_0', 'NR_UE_CCE_AggregationLev_0', 'NR_UE_Power_Tx_PUSCH_0', 'NR_UE_Power_Tx_PRACH_0', 'NR_UE_NACK_Rate_UL_0', 'NR_UE_RACH_Attempt', 'NR_UE_RACH_OK', 'NR_UE_RACH_Fail', 'NR_UE_RACH_Procedure_Count', 'NR_UE_RRCReEstAttempt', 'NR_UE_RRCReEstFail', 'NR_UE_RRCReEst_EndResult', 'NR_UE_RRCConnectionAttempt', 'NR_UE_RRCConnectionSetupOk', 'Unnamed: 48', 'NR_UE_RRCConnectionComplete', 'NR_UE_RRCConnectionDrop', 'NR_UE_RRCHOAttempt', 'NR_UE_RRCHOOK', 'NR_RRC_MsgType', 'NAS_5GS_MM_MessageType', 'NAS_5GS_SM_MessageType']\n",
      "\n",
      "[Step 2/6] Pre-processing data...\n",
      "  Converted 'Time' to datetime objects.\n",
      "  Converting signal columns to numeric and filling missing values...\n",
      "    Filled 688245 NaN values (originally non-numeric) in 'NR_UE_RSRP_0' with -140.0.\n",
      "    Filled 688245 NaN values (originally non-numeric) in 'NR_UE_RSRQ_0' with -120.0.\n",
      "    Filled 688255 NaN values (originally non-numeric) in 'NR_UE_SINR_0' with -20.0.\n",
      "\n",
      "[Step 3/6] Creating GeoDataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_7480\\3890240619.py:103: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[TIMESTAMP_COLUMN] = pd.to_datetime(df[TIMESTAMP_COLUMN])\n",
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_7480\\3890240619.py:129: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(default_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warning: Dropped 617710 rows with invalid or missing Latitude/Longitude values.\n",
      "  Successfully created GeoDataFrame with 72478 points.\n",
      "\n",
      "[Step 4/6] Generating static map ('signal_map_static.png')...\n",
      "  Adding basemap...\n",
      "  Basemap added successfully.\n",
      "  Static map saved as 'signal_map_static.png'.\n",
      "\n",
      "[Step 5/6] Performing Filtering and Querying...\n",
      "\n",
      "  --- Query 1: Data nearest to (40.7132, -74.0055) ---\n",
      "  Data point closest to target:\n",
      "    Actual Location (Lat, Lon): 41.10568, 29.01534\n",
      "    Distance (approx degrees): 103.021588\n",
      "    NR_UE_RSRP_0: -140.0 (Default)\n",
      "    NR_UE_RSRQ_0: -120.0 (Default)\n",
      "    NR_UE_SINR_0: -20.0 (Default)\n",
      "    Time: 2025-03-14 12:27:08\n",
      "\n",
      "  --- Query 2: Locations where NR_UE_RSRP_0 is between -100 and -90 dBm ---\n",
      "  Found 196 locations matching the criteria.\n",
      "  First 5 matching locations:\n",
      "      Latitude  Longitude  NR_UE_RSRP_0                Time\n",
      "490  41.10723   29.02949         -90.4 2025-03-14 12:14:40\n",
      "561  41.10722   29.02947         -90.1 2025-03-14 12:14:41\n",
      "633  41.10724   29.02944         -90.7 2025-03-14 12:14:42\n",
      "704  41.10725   29.02941         -91.9 2025-03-14 12:14:43\n",
      "746  41.10725   29.02941         -91.4 2025-03-14 12:14:43\n",
      "  Generating filtered static map ('signal_map_filtered.png')...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeric\\AppData\\Local\\Temp\\ipykernel_7480\\3890240619.py:239: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  distances = gdf.geometry.distance(target_point)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Filtered map saved as 'signal_map_filtered.png'.\n",
      "\n",
      "[Step 6/6] Generating interactive map ('signal_map_interactive.html')...\n",
      "  Interactive map saved as 'signal_map_interactive.html'. Open this file in your web browser.\n",
      "\n",
      "--- Script finished ---\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script to load geospatial signal data from CSV, handle missing values,\n",
    "plot locations on static and interactive maps, and perform filtering.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "from shapely.geometry import Point\n",
    "import folium\n",
    "import numpy as np # Used implicitly by pandas, good practice to import\n",
    "import sys # For exiting script gracefully\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration Section - MODIFY THESE VALUES\n",
    "# =============================================================================\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_FILE_PATH = 'dataset/5GDL.csv' # <<< CHANGE TO YOUR CSV FILE PATH\n",
    "LAT_COLUMN = 'Latitude'           # <<< CHANGE if your latitude column name is different\n",
    "LON_COLUMN = 'Longitude'          # <<< CHANGE if your longitude column name is different\n",
    "RSRP_COLUMN = 'NR_UE_RSRP_0'              # <<< CHANGE if your RSRP column name is different\n",
    "SINR_COLUMN = 'NR_UE_SINR_0'\n",
    "TIMESTAMP_COLUMN = 'Time'\n",
    "RSSI_COLUMN = 'NR_UE_RSRQ_0'\n",
    "# Add other columns you care about\n",
    "OTHER_COLUMNS = ['NR_UE_SINR_0', 'NR_UE_Timing_Advance', '']\n",
    "\n",
    "# --- Default Values for Missing Data ---\n",
    "# Choose values unlikely to occur naturally in your data\n",
    "DEFAULT_RSRP = -140.0\n",
    "DEFAULT_RSSI = -120.0\n",
    "DEFAULT_SINR = -20.0\n",
    "# Add defaults for other columns if they might be missing and numeric\n",
    "# DEFAULT_OTHER_SIGNAL = -999.0\n",
    "\n",
    "# --- Map Settings ---\n",
    "STATIC_MAP_FILENAME = 'signal_map_static.png'\n",
    "FILTERED_MAP_FILENAME = 'signal_map_filtered.png'\n",
    "INTERACTIVE_MAP_FILENAME = 'signal_map_interactive.html'\n",
    "DEFAULT_ZOOM_START = 13 # Zoom level for Folium map\n",
    "\n",
    "# --- Filtering Parameters ---\n",
    "# Example 1: Find data near this point\n",
    "TARGET_LATITUDE = 40.7132\n",
    "TARGET_LONGITUDE = -74.0055\n",
    "\n",
    "# Example 2: Filter by RSRP range\n",
    "MIN_RSRP_THRESHOLD = -100\n",
    "MAX_RSRP_THRESHOLD = -90\n",
    "\n",
    "# =============================================================================\n",
    "# Helper Function for Robust Numeric Conversion\n",
    "# =============================================================================\n",
    "def safe_to_numeric(series, column_name):\n",
    "    \"\"\"Converts a pandas Series to numeric, coercing errors, and reports issues.\"\"\"\n",
    "    original_dtype = series.dtype\n",
    "    numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "    num_nan_introduced = numeric_series.isna().sum() - series.isna().sum()\n",
    "    if num_nan_introduced > 0:\n",
    "        print(f\"  Warning: Column '{column_name}' had {num_nan_introduced} non-numeric values coerced to NaN.\")\n",
    "    if not pd.api.types.is_numeric_dtype(numeric_series) and not numeric_series.isna().all():\n",
    "         # This case is rare after 'coerce', but good practice\n",
    "         print(f\"  Warning: Column '{column_name}' could not be fully converted to numeric (original dtype: {original_dtype}). Check data.\")\n",
    "    return numeric_series\n",
    "\n",
    "# =============================================================================\n",
    "# Main Script Logic\n",
    "# =============================================================================\n",
    "print(\"--- Starting Signal Data Processing ---\")\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(f\"\\n[Step 1/6] Loading data from '{CSV_FILE_PATH}'...\")\n",
    "try:\n",
    "    df = pd.read_csv(CSV_FILE_PATH)\n",
    "    print(f\"  Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns.\")\n",
    "    print(\"  Columns found:\", df.columns.tolist())\n",
    "    # print(\"  First 5 rows (raw):\\n\", df.head()) # Uncomment for debugging\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"  Error: CSV file not found at '{CSV_FILE_PATH}'. Please check the path.\")\n",
    "    sys.exit(1) # Exit script\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An unexpected error occurred loading the CSV: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Verify Essential Columns Exist ---\n",
    "required_columns = [LAT_COLUMN, LON_COLUMN]\n",
    "if RSRP_COLUMN: required_columns.append(RSRP_COLUMN)\n",
    "missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"  Error: Missing essential columns in CSV: {missing_cols}. Check configuration.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- 2. Pre-process Data (Timestamp, Numeric Conversion, Fill NaNs) ---\n",
    "print(\"\\n[Step 2/6] Pre-processing data...\")\n",
    "\n",
    "# Convert Timestamp\n",
    "if TIMESTAMP_COLUMN and TIMESTAMP_COLUMN in df.columns:\n",
    "    try:\n",
    "        df[TIMESTAMP_COLUMN] = pd.to_datetime(df[TIMESTAMP_COLUMN])\n",
    "        print(f\"  Converted '{TIMESTAMP_COLUMN}' to datetime objects.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not parse timestamp column '{TIMESTAMP_COLUMN}'. Error: {e}. Skipping conversion.\")\n",
    "else:\n",
    "     if TIMESTAMP_COLUMN:\n",
    "         print(f\"  Info: Timestamp column '{TIMESTAMP_COLUMN}' not found in CSV.\")\n",
    "\n",
    "# Define columns to fill and their default values\n",
    "columns_to_fill = {}\n",
    "if RSRP_COLUMN in df.columns: columns_to_fill[RSRP_COLUMN] = DEFAULT_RSRP\n",
    "if RSSI_COLUMN in df.columns: columns_to_fill[RSSI_COLUMN] = DEFAULT_RSSI\n",
    "if SINR_COLUMN in df.columns: columns_to_fill[SINR_COLUMN] = DEFAULT_SINR\n",
    "# Add other numeric columns needing defaults here:\n",
    "# if 'OtherSignal' in df.columns: columns_to_fill['OtherSignal'] = DEFAULT_OTHER_SIGNAL\n",
    "\n",
    "# Apply numeric conversion and fill NaNs\n",
    "print(\"  Converting signal columns to numeric and filling missing values...\")\n",
    "for col, default_val in columns_to_fill.items():\n",
    "    if col in df.columns:\n",
    "        nan_count_before = df[col].isna().sum()\n",
    "        # Convert to numeric first, handling potential non-numeric entries\n",
    "        df[col] = safe_to_numeric(df[col], col)\n",
    "        nan_count_after_coerce = df[col].isna().sum()\n",
    "\n",
    "        if nan_count_after_coerce > 0:\n",
    "            df[col].fillna(default_val, inplace=True)\n",
    "            filled_count = nan_count_after_coerce - nan_count_before # Count NaNs filled (original + coerced)\n",
    "            if filled_count > 0 :\n",
    "                 print(f\"    Filled {filled_count} NaN/invalid values in '{col}' with {default_val}.\")\n",
    "            else:\n",
    "                 # This happens if all NaNs were introduced by coercion and then filled\n",
    "                 if nan_count_after_coerce > 0:\n",
    "                      print(f\"    Filled {nan_count_after_coerce} NaN values (originally non-numeric) in '{col}' with {default_val}.\")\n",
    "        # else: # No NaNs found or introduced\n",
    "        #     print(f\"    No NaN values found or filled in '{col}'.\") # Can be noisy, often omitted\n",
    "    else:\n",
    "        print(f\"  Warning: Column '{col}' specified for filling not found in DataFrame.\")\n",
    "\n",
    "# print(\"  First 5 rows (after processing):\\n\", df.head()) # Uncomment for debugging\n",
    "\n",
    "# --- 3. Create GeoDataFrame ---\n",
    "print(\"\\n[Step 3/6] Creating GeoDataFrame...\")\n",
    "try:\n",
    "    # Ensure Lat/Lon are numeric, drop rows if not convertible\n",
    "    df[LAT_COLUMN] = safe_to_numeric(df[LAT_COLUMN], LAT_COLUMN)\n",
    "    df[LON_COLUMN] = safe_to_numeric(df[LON_COLUMN], LON_COLUMN)\n",
    "\n",
    "    original_rows = len(df)\n",
    "    df.dropna(subset=[LAT_COLUMN, LON_COLUMN], inplace=True)\n",
    "    rows_dropped = original_rows - len(df)\n",
    "    if rows_dropped > 0:\n",
    "        print(f\"  Warning: Dropped {rows_dropped} rows with invalid or missing Latitude/Longitude values.\")\n",
    "\n",
    "    if df.empty:\n",
    "         print(\"  Error: No valid location data remaining after cleaning Lat/Lon. Cannot proceed.\")\n",
    "         sys.exit(1)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[LON_COLUMN], df[LAT_COLUMN]),\n",
    "        crs=\"EPSG:4326\" # WGS84 Coordinate Reference System\n",
    "    )\n",
    "    print(f\"  Successfully created GeoDataFrame with {len(gdf)} points.\")\n",
    "    # print(gdf.head()) # Uncomment for debugging\n",
    "\n",
    "except KeyError as e:\n",
    "     print(f\"  Error: Missing required column for GeoDataFrame creation: {e}. Check LAT_COLUMN/LON_COLUMN names.\")\n",
    "     sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An unexpected error occurred creating the GeoDataFrame: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- 4. Plotting: Static Map ---\n",
    "print(f\"\\n[Step 4/6] Generating static map ('{STATIC_MAP_FILENAME}')...\")\n",
    "try:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "\n",
    "    # Determine color range, potentially excluding the default fill value\n",
    "    color_col = RSRP_COLUMN\n",
    "    plot_column_data = gdf[color_col]\n",
    "    default_val = columns_to_fill.get(color_col) # Get default if defined\n",
    "\n",
    "    # Calculate vmin/vmax from actual data, ignoring default fill value if specified\n",
    "    valid_data_for_range = plot_column_data\n",
    "    if default_val is not None:\n",
    "        valid_data_for_range = plot_column_data[plot_column_data != default_val]\n",
    "\n",
    "    vmin = valid_data_for_range.min() if not valid_data_for_range.empty else plot_column_data.min()\n",
    "    vmax = valid_data_for_range.max() if not valid_data_for_range.empty else plot_column_data.max()\n",
    "\n",
    "\n",
    "    gdf.plot(\n",
    "        column=color_col,\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        markersize=15,\n",
    "        cmap='viridis', # Good perceptually uniform colormap\n",
    "        vmin=vmin,      # Set color limits based on data range\n",
    "        vmax=vmax,\n",
    "        legend_kwds={'label': f\"{color_col} (dBm)\",\n",
    "                     'orientation': \"horizontal\"}\n",
    "    )\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        print(\"  Adding basemap...\")\n",
    "        ctx.add_basemap(ax, crs=gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik, zoom='auto')\n",
    "        print(\"  Basemap added successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not add basemap. Plot will show points only. Error: {e}\")\n",
    "\n",
    "    ax.set_title(f'Signal Measurement Locations colored by {color_col}')\n",
    "    ax.set_axis_off() # Hide lat/lon axes if basemap is present\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(STATIC_MAP_FILENAME, dpi=300)\n",
    "    print(f\"  Static map saved as '{STATIC_MAP_FILENAME}'.\")\n",
    "    # plt.show() # Optionally display the plot directly\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An error occurred during static plotting: {e}\")\n",
    "\n",
    "plt.close(fig) # Close the figure to free memory\n",
    "\n",
    "# --- 5. Filtering and Querying ---\n",
    "print(\"\\n[Step 5/6] Performing Filtering and Querying...\")\n",
    "\n",
    "# Example 1: Find RSRP (and other data) near a specific Lat/Lon\n",
    "print(f\"\\n  --- Query 1: Data nearest to ({TARGET_LATITUDE}, {TARGET_LONGITUDE}) ---\")\n",
    "try:\n",
    "    target_point = Point(TARGET_LONGITUDE, TARGET_LATITUDE)\n",
    "    # Ensure target point is valid\n",
    "    if not target_point.is_valid:\n",
    "         print(f\"  Error: Invalid target coordinates specified.\")\n",
    "    else:\n",
    "        # Calculate distances (in degrees for EPSG:4326)\n",
    "        distances = gdf.geometry.distance(target_point)\n",
    "\n",
    "        if not distances.empty:\n",
    "            nearest_index = distances.idxmin()\n",
    "            nearest_data = gdf.loc[nearest_index]\n",
    "\n",
    "            print(f\"  Data point closest to target:\")\n",
    "            print(f\"    Actual Location (Lat, Lon): {nearest_data.geometry.y:.5f}, {nearest_data.geometry.x:.5f}\")\n",
    "            print(f\"    Distance (approx degrees): {distances.min():.6f}\") # Note: Degree distance != meters!\n",
    "            # Display key values, checking if they exist and handling defaults\n",
    "            for col, default in columns_to_fill.items():\n",
    "                 if col in nearest_data:\n",
    "                     val = nearest_data[col]\n",
    "                     print(f\"    {col}: {val}{' (Default)' if val == default else ''}\")\n",
    "            # Display timestamp if available\n",
    "            if TIMESTAMP_COLUMN and TIMESTAMP_COLUMN in nearest_data and pd.notna(nearest_data[TIMESTAMP_COLUMN]):\n",
    "                 print(f\"    {TIMESTAMP_COLUMN}: {nearest_data[TIMESTAMP_COLUMN]}\")\n",
    "            # Display other configured columns\n",
    "            # for col in OTHER_DATA_COLUMNS:\n",
    "            #      if col in nearest_data and col not in columns_to_fill: # Avoid printing twice\n",
    "            #           print(f\"    {col}: {nearest_data[col]}\")\n",
    "        else:\n",
    "            print(f\"  No data points found to calculate nearest distance.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An error occurred finding the nearest point: {e}\")\n",
    "\n",
    "\n",
    "# Example 2: Find locations where RSRP is within a certain range\n",
    "print(f\"\\n  --- Query 2: Locations where {RSRP_COLUMN} is between {MIN_RSRP_THRESHOLD} and {MAX_RSRP_THRESHOLD} dBm ---\")\n",
    "if RSRP_COLUMN not in gdf.columns:\n",
    "     print(f\"  Skipping query: RSRP column '{RSRP_COLUMN}' not found.\")\n",
    "else:\n",
    "    try:\n",
    "        # Apply the filter using boolean indexing\n",
    "        filtered_gdf = gdf[\n",
    "            (gdf[RSRP_COLUMN] >= MIN_RSRP_THRESHOLD) &\n",
    "            (gdf[RSRP_COLUMN] <= MAX_RSRP_THRESHOLD)\n",
    "        ]\n",
    "\n",
    "        print(f\"  Found {len(filtered_gdf)} locations matching the criteria.\")\n",
    "\n",
    "        if not filtered_gdf.empty:\n",
    "            # Display first few matching locations\n",
    "            display_cols = [LAT_COLUMN, LON_COLUMN, RSRP_COLUMN]\n",
    "            if TIMESTAMP_COLUMN in filtered_gdf.columns: display_cols.append(TIMESTAMP_COLUMN)\n",
    "            print(\"  First 5 matching locations:\\n\", filtered_gdf[display_cols].head())\n",
    "\n",
    "            # Optional: Plot only the filtered points on a new map\n",
    "            print(f\"  Generating filtered static map ('{FILTERED_MAP_FILENAME}')...\")\n",
    "            try:\n",
    "                fig_filtered, ax_filtered = plt.subplots(1, 1, figsize=(10, 10))\n",
    "                # Plot all points faintly for context\n",
    "                gdf.plot(ax=ax_filtered, color='grey', markersize=5, alpha=0.3, label='All Data')\n",
    "                # Highlight filtered points\n",
    "                filtered_gdf.plot(ax=ax_filtered, color='red', markersize=25, label=f'{RSRP_COLUMN} {MIN_RSRP_THRESHOLD} to {MAX_RSRP_THRESHOLD}')\n",
    "                # Add Basemap\n",
    "                try:\n",
    "                    ctx.add_basemap(ax_filtered, crs=gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik, zoom='auto')\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: Could not add basemap to filtered plot. Error: {e}\")\n",
    "\n",
    "                ax_filtered.set_title(f'Locations with {RSRP_COLUMN} in range [{MIN_RSRP_THRESHOLD}, {MAX_RSRP_THRESHOLD}] dBm')\n",
    "                ax_filtered.set_axis_off()\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(FILTERED_MAP_FILENAME, dpi=300)\n",
    "                print(f\"  Filtered map saved as '{FILTERED_MAP_FILENAME}'.\")\n",
    "                # plt.show() # Optionally display\n",
    "                plt.close(fig_filtered) # Close figure\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: An error occurred plotting the filtered map: {e}\")\n",
    "                if 'fig_filtered' in locals(): plt.close(fig_filtered) # Ensure cleanup if error occurred mid-plot\n",
    "\n",
    "        else:\n",
    "            print(\"  No locations matched the filtering criteria.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: An error occurred during RSRP filtering: {e}\")\n",
    "\n",
    "\n",
    "# --- 6. Plotting: Interactive Map (Folium) ---\n",
    "print(f\"\\n[Step 6/6] Generating interactive map ('{INTERACTIVE_MAP_FILENAME}')...\")\n",
    "try:\n",
    "    # Create map centered around the mean location\n",
    "    map_center = [gdf[LAT_COLUMN].mean(), gdf[LON_COLUMN].mean()]\n",
    "    interactive_map = folium.Map(location=map_center, zoom_start=DEFAULT_ZOOM_START)\n",
    "\n",
    "    # Add points to the map\n",
    "    for idx, row in gdf.iterrows():\n",
    "        # Create popup text with relevant info\n",
    "        popup_html = f\"<b><u>Location {idx}</u></b><br>\"\n",
    "        popup_html += f\"<b>Lat:</b> {row[LAT_COLUMN]:.5f}<br>\"\n",
    "        popup_html += f\"<b>Lon:</b> {row[LON_COLUMN]:.5f}<br>\"\n",
    "\n",
    "        # Add signal values, indicating if it was a default fill\n",
    "        all_signal_cols = list(columns_to_fill.keys()) # Includes RSRP, RSSI, SINR etc. if defined\n",
    "        for col in all_signal_cols:\n",
    "            if col in row:\n",
    "                 val = row[col]\n",
    "                 default_val = columns_to_fill.get(col) # Get the default for comparison\n",
    "                 popup_html += f\"<b>{col}:</b> {val}{' (Default)' if val == default_val else ''}<br>\"\n",
    "\n",
    "        # Add other data columns specified\n",
    "        # for col in OTHER_DATA_COLUMNS:\n",
    "        #     if col in row and col not in columns_to_fill: # Avoid adding signals twice\n",
    "        #          popup_html += f\"<b>{col}:</b> {row[col]}<br>\"\n",
    "\n",
    "        # Add timestamp if available\n",
    "        if TIMESTAMP_COLUMN and TIMESTAMP_COLUMN in row and pd.notna(row[TIMESTAMP_COLUMN]):\n",
    "             popup_html += f\"<b>{TIMESTAMP_COLUMN}:</b> {row[TIMESTAMP_COLUMN]}\"\n",
    "\n",
    "        # Determine marker color based on RSRP (customize this logic if needed)\n",
    "        rsrp_val = row.get(RSRP_COLUMN) # Use .get for safety if RSRP_COLUMN somehow missing\n",
    "        rsrp_default = columns_to_fill.get(RSRP_COLUMN)\n",
    "        color = 'grey' # Default/Unknown\n",
    "\n",
    "        if rsrp_val is not None:\n",
    "            if rsrp_val == rsrp_default:\n",
    "                 color = 'darkgrey' # Specific color for default/missing RSRP\n",
    "            elif rsrp_val > -95:\n",
    "                 color = 'green'\n",
    "            elif rsrp_val > -105:\n",
    "                 color = 'orange'\n",
    "            elif rsrp_val <= -105: # Catches values below -105\n",
    "                 color = 'red'\n",
    "            # Add more elif conditions for finer granularity if desired\n",
    "\n",
    "        # Add marker to map\n",
    "        folium.CircleMarker(\n",
    "            location=[row[LAT_COLUMN], row[LON_COLUMN]],\n",
    "            radius=5, # Adjust size\n",
    "            popup=folium.Popup(popup_html, max_width=300),\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.7\n",
    "        ).add_to(interactive_map)\n",
    "\n",
    "    # Save the map to an HTML file\n",
    "    interactive_map.save(INTERACTIVE_MAP_FILENAME)\n",
    "    print(f\"  Interactive map saved as '{INTERACTIVE_MAP_FILENAME}'. Open this file in your web browser.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error: An error occurred generating the interactive map: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Script finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
